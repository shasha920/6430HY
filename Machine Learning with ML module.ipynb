{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBA 6430 - Big Data Technology in Business\n",
    "# Dr. Mohammad Salehan\n",
    "# Machine Learning with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T21:31:07.057495Z",
     "iopub.status.busy": "2022-06-28T21:31:07.057189Z"
    }
   },
   "source": [
    "As of Spark 2.0, the RDD-based APIs in the spark.mllib package have entered maintenance mode. The primary Machine Learning API for Spark is now the DataFrame-based API in the spark.ml package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get a cell_monitor error, you can ignore it. It is a Jypyter cell error and not a Spark error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset information: Predicting forest cover type from cartographic variables.  The actual forest cover type for a given observation (30 x 30 meter cell) was determined from US Forest Service (USFS) Region 2 Resource Information System (RIS) data. <a href='https://archive.ics.uci.edu/ml/datasets/covertype'>Data Description</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "import matplotlib.pyplot as plt\n",
    "ps.set_option('plotting.backend', 'matplotlib')\n",
    "\n",
    "forest_path = 's3://cis4567-salehan/Spark/Data/forest_coverage_type.csv'\n",
    "forest = ps.read_csv(forest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forest.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pretty\n",
    "forest.to_spark().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of the most popular **Transformers**\n",
    "* Binarizer\n",
    "* Bucketizer\n",
    "* ChiSqSelector\n",
    "* CountVectorizer\n",
    "* ElementwiseProduct\n",
    "* HashingTF\n",
    "* IDF\n",
    "* IndexToString\n",
    "* MaxAbsScaler\n",
    "* MinMaxScaler\n",
    "* NGram\n",
    "* Normalizer\n",
    "* OneHotEncoder\n",
    "* PCA\n",
    "* QuantileDiscretizer\n",
    "* RegexTokenizer\n",
    "* SQLTransformer\n",
    "* StandardScaler\n",
    "* StopWordsRemover\n",
    "* StringIndexer\n",
    "* Tokenizer\n",
    "* VectorAssembler\n",
    "* Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.ml.feature as feat\n",
    "from pyspark.ml import Pipeline\n",
    "import numpy as np\n",
    "import pyspark.ml.classification as cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components Analysis\n",
    "* PCA is a dimension reduction technique.\n",
    "<img src='https://miro.medium.com/max/462/1*QALwLkPJG45E7_DsOccdaw.png'>\n",
    "* Almost exclusively, every estimator (or, in other words, an ML model) found in the MLlib module expects to see a single column as an input; the column should contain all the features a data scientist wants such a model to use. \n",
    "* The .VectorAssembler(...) method, as the name suggests, collates multiple features into a single column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorAssembler = (\n",
    "    feat.VectorAssembler(\n",
    "        inputCols=list(forest.columns), \n",
    "        outputCol='feat'\n",
    "    )\n",
    ")\n",
    "\n",
    "norm = feat.StandardScaler(\n",
    "    inputCol=vectorAssembler.getOutputCol()\n",
    "    , outputCol='norm'\n",
    "    , withMean=True\n",
    "    , withStd=True\n",
    ")\n",
    "\n",
    "#fit PCA to the vector with 5 components\n",
    "pca = (\n",
    "    feat.PCA(\n",
    "        k=25\n",
    "        , inputCol=norm.getOutputCol()\n",
    "        , outputCol='pca_feat'\n",
    "    )\n",
    ")\n",
    "\n",
    "#transform the data using PCA\n",
    "#original dataset contains 55 columsn\n",
    "#the output shows one vectorized record as well as\n",
    "#the 5 feature output of the record from PCA\n",
    "pca_pipeline = Pipeline(stages=[vectorAssembler, norm, pca])\n",
    "pModel = (\n",
    "    pca_pipeline\n",
    "    .fit(forest.to_spark())\n",
    ")\n",
    "\n",
    "(\n",
    "    pModel.\n",
    "    transform(forest.to_spark()).\n",
    "    pandas_api()\n",
    "    [['feat','pca_feat']]\n",
    "    .head(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pModel.stages[-1].explainedVariance)\n",
    "print(np.cumsum(pModel.stages[-1].explainedVariance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of the most popular **Estimators**\n",
    "1. Classification\n",
    " * LinearSVC\n",
    " * LogisticRegression \n",
    " * DecisionTreeClassifier\n",
    " * GBTClassifier\n",
    " * RandomForestClassifier\n",
    " * NaiveBayes\n",
    " * MultilayerPerceptronClassifier\n",
    " * OneVsRest\n",
    "2. Regression\n",
    " * LinearRegression\n",
    " * GeneralizedLinearRegression\n",
    " * DecisionTreeRegressor\n",
    " * GBTRegressor\n",
    " * RandomForestRegressor\n",
    "3. Clustering\n",
    " * Kmeans\n",
    " * LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#examine all distinct values for outcome variable\n",
    "forest['CoverType'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the label\n",
    "We transform the labels to start at zero (instead of one) otherwise classification models think there are 8 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest['CoverType'] = forest['CoverType']-1\n",
    "forest['CoverType'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Support Vector Machine (SVM)\n",
    "* SVM is a classification and regression technique that tries to separate the two classes using a hyperplane. \n",
    "* SVM algorithm finds the points closest to the line from both the classes.These points are called support vectors. \n",
    "* Then, it compute the distance between the line and the support vectors. This distance is called the margin. \n",
    "* The goal is to maximize the margin. \n",
    "* Watch <a href='https://www.youtube.com/watch?v=Y6RRHw9uN9o'>this video </a> to learn more about SVM.  \n",
    "<center><img src='https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/300px-SVM_margin.png'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a vector of all variables except for the dependent variable\n",
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols=[x for x in forest.columns if x != 'CoverType']\n",
    "    , outputCol='features')\n",
    "\n",
    "#We will predict the forest cover type equal to 1, that is, \n",
    "#whether the forest is a spruce-fir type; we achieve this by \n",
    "#checking whether CoverType is equal to 1 and \n",
    "#casting the resulting Boolean to an integer (0: false, 1: true)\n",
    "fir_dataset = (\n",
    "    vectorAssembler\n",
    "    .transform(forest.to_spark())\n",
    "    .pandas_api()\n",
    "    .assign(label=(lambda x: (x['CoverType']==0).astype('int')))\n",
    "    [['label', 'features']]\n",
    ")\n",
    "\n",
    "fir_dataset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a SVM object and fit to data\n",
    "#the default value of label coulmn is label\n",
    "#regParam is regularization parameter. In this case it will be L2\n",
    "svc_obj = cl.LinearSVC(maxIter=10, regParam=0.01)\n",
    "svc_model = svc_obj.fit(fir_dataset.to_spark())\n",
    "\n",
    "#examine the model coefficients. SVM is a linear model.\n",
    "svc_model.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "In MLlib, we implement popular linear methods such as logistic regression and linear least squares with L1 or L2 regularization. In spark.ml, we also include Pipelines API for Elastic net, a hybrid of L1 and L2 regularization via the elastic net. Mathematically, it is defined as a convex combination of the L1 and the L2 regularization terms:"
   ]
  },
  {
   "attachments": {
    "Elastic_Net.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Elastic_Net.png](attachment:Elastic_Net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ElasticNetParam corresponds to α and regParam corresponds to λ. The default value for both is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "forest['Elevation'].hist()\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.regression as rg\n",
    "\n",
    "# let's predict elevation (first column) using the rest of features\n",
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols=[x for x in forest.columns if x != 'Elevation']\n",
    "    , outputCol='features')\n",
    "\n",
    "#create a linear regression object and fit to dataset    \n",
    "lr_obj = rg.LinearRegression(\n",
    "    labelCol='Elevation',\n",
    "    maxIter=10\n",
    "    , regParam=0.01\n",
    "    , elasticNetParam=1.00)\n",
    "# lr_model = lr_obj.fit(elevation_dataset.to_spark())\n",
    "\n",
    "#examine model coefficients\n",
    "pip = Pipeline(stages=[vectorAssembler, lr_obj])\n",
    "\n",
    "#run the pipeline\n",
    "pModel = pip.fit(forest.to_spark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the trained model from the pipeline\n",
    "lr_model = pModel.stages[-1]\n",
    "#examine model coefficients\n",
    "lr_model.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = lr_model.summary\n",
    "\n",
    "print(\n",
    "    summary.r2\n",
    "    , summary.rootMeanSquaredError\n",
    "    , summary.meanAbsoluteError\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predictions from the model\n",
    "(\n",
    "    pModel.transform(forest.to_spark())\n",
    "    .pandas_api()\n",
    "    [['Elevation', 'prediction']]\n",
    "    .head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the most predictable features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark.ml.stat as st\n",
    "import pandas as pd\n",
    "\n",
    "features_and_label = feat.VectorAssembler(\n",
    "    inputCols=list(forest.columns)\n",
    "    , outputCol='features'\n",
    ")\n",
    "\n",
    "corr = st.Correlation.corr(\n",
    "    features_and_label.transform(forest.to_spark()), \n",
    "    'features', \n",
    "    'pearson'\n",
    ")\n",
    "\n",
    "corr_pd = corr.toPandas()\n",
    "output_np = np.array(corr_pd.iloc[0, 0].values).reshape(\n",
    "    (corr_pd.iloc[0, 0].numRows, corr_pd.iloc[0, 0].numCols))\n",
    "\n",
    "corr_pd = pd.DataFrame(output_np, columns=forest.columns)\n",
    "corr_pd.index = forest.columns\n",
    "tril = np.tril(corr_pd)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.8, wspace=0.2, hspace=0.2)\n",
    "\n",
    "cax = ax.matshow(tril, cmap='coolwarm_r')\n",
    "fig.colorbar(cax)\n",
    "\n",
    "plt.xticks(range(len(corr_pd.columns)), corr_pd.columns, rotation=90, fontsize=6)\n",
    "plt.yticks(range(len(corr_pd.columns)), corr_pd.columns, fontsize=6)\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the correlation matrix to S3\n",
    "corr_pd = ps.from_pandas(corr_pd)\n",
    "corr_pd.to_spark(index_col='index').repartition(1).pandas_api().to_csv('s3://gba6430-salehan-private/spark/tmp/corr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Feature Selector\n",
    "is used for model selection. It selects a smaller subset of features which explain the dependent variable well.\n",
    "* featureType categorical and labelType categorical, Spark uses `Chi-Squared`, i.e. Chi<sup>2</sup> in sklearn.\n",
    "* featureType continuous and labelType categorical, Spark uses `ANOVA F-test`, i.e. f_classif in sklearn.\n",
    "* featureType continuous and labelType continuous, Spark uses `F-value`, i.e. f_regression in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude the label CoverType from features vector\n",
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols=[x for x in forest.columns if x != 'CoverType']\n",
    "    , outputCol='features'\n",
    ")\n",
    "\n",
    "#select top 10 features, store in a new column named selected\n",
    "selector = feat.UnivariateFeatureSelector(\n",
    "    labelCol='CoverType'\n",
    "    , outputCol='selected'\n",
    "    , selectionMode = 'numTopFeatures'\n",
    "    ).setFeatureType(\"categorical\"\n",
    "    ).setLabelType(\"categorical\"\n",
    "    ).setSelectionThreshold(10) #select top 10 features\n",
    "\n",
    "pipeline_sel = Pipeline(stages=[vectorAssembler, selector])\n",
    "\n",
    "model = (\n",
    "    pipeline_sel\n",
    "    .fit(forest.to_spark())\n",
    "    .transform(forest.to_spark())\n",
    ")\n",
    "\n",
    "#print selected features\n",
    "model.schema['selected'].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display selected features as a dataframe\n",
    "pd.DataFrame(model.schema['selected'].metadata['ml_attr']['attrs']['numeric'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting forest coverage type\n",
    "### Multinomial classification using binary classifiers, one vs the rest technique \n",
    "<img src='https://miro.medium.com/max/972/1*SwXHlCzh-d9UqHOglp3vcA.png' />  \n",
    "<a href='https://medium.com/@b.terryjack/tips-and-tricks-for-multi-class-classification-c184ae1c8ffc'>Image source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and test sets\n",
    "forest_train, forest_test = (\n",
    "    forest.to_spark()\n",
    "    .randomSplit([0.7, 0.3], seed=666)\n",
    ")\n",
    "\n",
    "#convert all features ti vector except for last column\n",
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols=[x for x in forest.columns if x != 'CoverType']\n",
    "    , outputCol='features'\n",
    ")\n",
    "\n",
    "#select top 10 predictors\n",
    "selector = feat.UnivariateFeatureSelector(\n",
    "    labelCol='CoverType'\n",
    "    , outputCol='selected'\n",
    "    , selectionMode = 'numTopFeatures'\n",
    "    ).setFeatureType(\"categorical\"\n",
    "    ).setLabelType(\"categorical\"\n",
    "    ).setSelectionThreshold(10) #select top 10 features\n",
    "\n",
    "#create a multinomial regression object\n",
    "#DV has 7 classes\n",
    "logReg_obj = cl.LogisticRegression(\n",
    "    labelCol='CoverType'\n",
    "    , featuresCol=selector.getOutputCol()\n",
    "    , regParam=0.01\n",
    "    , elasticNetParam=1.0\n",
    "    , family='multinomial'\n",
    ")\n",
    "\n",
    "#create pipeline of 1)vector, 2)chisquare selector, 3)logistic regression\n",
    "pipeline = Pipeline(\n",
    "    stages=[\n",
    "        vectorAssembler\n",
    "        , selector\n",
    "        , logReg_obj\n",
    "    ])\n",
    "#train the model\n",
    "pModel = pipeline.fit(forest_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check out the Logistic Regression object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pModel.stages[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.evaluation as ev\n",
    "\n",
    "#apply the model to test set\n",
    "logReg_model = (\n",
    "    pModel\n",
    "    .transform(forest_test)\n",
    ")\n",
    "\n",
    "results_logReg = logReg_model.select('CoverType', 'probability', 'prediction')\n",
    "\n",
    "#evaluate performance of the multinomial model (7 classes)\n",
    "evaluator = ev.MulticlassClassificationEvaluator(\n",
    "    predictionCol='prediction'\n",
    "    , labelCol='CoverType')\n",
    "\n",
    "(\n",
    "    #F-1 score\n",
    "    evaluator.evaluate(results_logReg)\n",
    "    #weighted precision\n",
    "    , evaluator.evaluate(\n",
    "        results_logReg\n",
    "        , {evaluator.metricName: 'weightedPrecision'}\n",
    "    ) \n",
    "    #accuracy\n",
    "    , evaluator.evaluate(\n",
    "        results_logReg\n",
    "        , {evaluator.metricName: 'accuracy'}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print selected features from chisquare selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pretty\n",
    "selected_cols = ([x['name'] for x in logReg_model.schema['selected'].metadata['ml_attr']['attrs']['numeric']])\n",
    "\n",
    "#print model coefficients\n",
    "#for a binomial model use coefficients instead of coefficientMatrix\n",
    "pd.set_option('display.max_columns', 10)\n",
    "ps.DataFrame(pModel.stages[-1].coefficientMatrix.toArray(), columns=selected_cols).to_spark().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods: Bagging VS Boosting\n",
    "#### bagging\n",
    "<img src='https://miro.medium.com/max/2000/1*_B5HX2whbTs3DS8M6YBD_w@2x.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting \n",
    "<img src=\"https://miro.medium.com/max/2000/1*VGSoqefx3Rz5Pws6qpLwOQ@2x.png\" />\n",
    "<a href='https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205'>More information about ensemble methods</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted measures are weighted based on the size of each class. Read about multinomial classification performance <a href='https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1'>here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as previous, this time using Random Forest\n",
    "rf_obj = cl.RandomForestClassifier(\n",
    "    labelCol='CoverType'\n",
    "    , featuresCol=selector.getOutputCol()\n",
    "    , minInstancesPerNode=10\n",
    "    , numTrees=10\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages=[vectorAssembler, selector, rf_obj]\n",
    ")\n",
    "\n",
    "pModel = pipeline.fit(forest_train)\n",
    "\n",
    "rf_obj_trained = (\n",
    "    pModel\n",
    "    .transform(forest_test)\n",
    ")\n",
    "results_rf = rf_obj_trained.select('CoverType', 'probability', 'prediction')\n",
    "\n",
    "evaluator = ev.MulticlassClassificationEvaluator(\n",
    "    predictionCol='prediction'\n",
    "    , labelCol='CoverType')\n",
    "\n",
    "(\n",
    "    evaluator.evaluate(results_rf)\n",
    "    , evaluator.evaluate(\n",
    "        results_rf\n",
    "        , {evaluator.metricName: 'weightedPrecision'}\n",
    "    )\n",
    "    , evaluator.evaluate(\n",
    "        results_rf\n",
    "        , {evaluator.metricName: 'accuracy'}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print selected features from chisquare selector\n",
    "ps.DataFrame(rf_obj_trained.schema['selected'].metadata['ml_attr']['attrs']['numeric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the feature importances\n",
    "feature_importances = pModel.stages[-1].featureImportances\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in zip(forest_train.columns, feature_importances):\n",
    "    print(feature, \": \", importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating forest elevation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols=[x for x in forest.columns if x != 'Elevation']\n",
    "    , outputCol='features')\n",
    "\n",
    "#minInfoGain: min decrease in impurity to split\n",
    "rf_obj = rg.RandomForestRegressor(\n",
    "    labelCol='Elevation'\n",
    "    , maxDepth=10\n",
    "    , minInstancesPerNode=10\n",
    "    , minInfoGain=0.1\n",
    "    , numTrees=10\n",
    ")\n",
    "\n",
    "pip = Pipeline(stages=[vectorAssembler, rf_obj])\n",
    "\n",
    "results = (\n",
    "    pip\n",
    "    .fit(forest.to_spark())\n",
    "    .transform(forest.to_spark())\n",
    "    .select('Elevation', 'prediction')\n",
    ")\n",
    "\n",
    "evaluator = ev.RegressionEvaluator(labelCol='Elevation')\n",
    "evaluator.evaluate(results, {evaluator.metricName: 'r2'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Trees regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_obj = rg.GBTRegressor(\n",
    "    labelCol='Elevation'\n",
    "    , minInstancesPerNode=10\n",
    "    , minInfoGain=0.1\n",
    ")\n",
    "\n",
    "pip = Pipeline(stages=[vectorAssembler, gbt_obj])\n",
    "\n",
    "results = (\n",
    "    pip\n",
    "    .fit(forest.to_spark())\n",
    "    .transform(forest.to_spark())\n",
    "    .select('Elevation', 'prediction')\n",
    ")\n",
    "\n",
    "evaluator = ev.RegressionEvaluator(labelCol='Elevation')\n",
    "evaluator.evaluate(results, {evaluator.metricName: 'r2'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering forest cover type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.clustering as clust\n",
    "\n",
    "#include all features except for the label\n",
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols=[x for x in forest.columns if x != 'CoverType']\n",
    "    , outputCol='features')\n",
    "\n",
    "norm = feat.StandardScaler(\n",
    "    inputCol=vectorAssembler.getOutputCol()\n",
    "    , outputCol='standardized'\n",
    "    , withMean=True\n",
    "    , withStd=True\n",
    ")\n",
    "\n",
    "#KMeans with k = 7\n",
    "#set distance measure to euclidean or cosine\n",
    "distance_measure = 'cosine'\n",
    "kmeans_obj = clust.KMeans(k=7, \n",
    "                          seed=666, \n",
    "                          featuresCol=norm.getOutputCol(), \n",
    "                          distanceMeasure=distance_measure)\n",
    "\n",
    "pip = Pipeline(stages=[vectorAssembler, norm, kmeans_obj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_results = (\n",
    "    pip\n",
    "    .fit(forest.to_spark())\n",
    "    .transform(forest.to_spark())\n",
    "    .pandas_api()\n",
    ")\n",
    "\n",
    "clustering_results[['CoverType', 'prediction']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculate the Silhouette metric for clustering model  \n",
    "* Silhouette ranges from −1 to +1  \n",
    "* Anything around 0.5 or more indicates well separated clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_distance_measure = 'squaredEuclidean' if distance_measure == 'euclidean' else distance_measure\n",
    "clustering_ev = ev.ClusteringEvaluator(featuresCol=norm.getOutputCol(), distanceMeasure=eval_distance_measure)\n",
    "clustering_ev.evaluate(clustering_results.to_spark())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clustering_results.prediction.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_scatter(df, x, y, color):\n",
    "    plt.clf()\n",
    "    ax = df.plot.scatter(x, y, c=color, colormap='viridis')\n",
    "    ax.set_xlabel(x)\n",
    "    ax.set_ylabel(y)\n",
    "    ax.set_title(f'Clustering {x} vs {y}');\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_0_and_1 = clustering_results[clustering_results.prediction.isin([0,1])]\n",
    "cluster_scatter(clusters_0_and_1, 'Elevation', 'Aspect', 'prediction')\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_3_and_1 = clustering_results[clustering_results.prediction.isin([3,1])]\n",
    "cluster_scatter(clusters_3_and_1, 'Aspect', 'Slope', 'prediction')\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_results.groupby('prediction').agg({**{x:'mean' for x in ['Elevation', 'Aspect', 'Slope']}, \n",
    "                                              'features':'count'}).rename(\n",
    "                                                {'features':'count'}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search\n",
    "* Grid search allows us to test different values for hyper parameters, such as learning rate, regularization parameter, and elestic net parameter, and select the best values. Running the following cell will take several minutes.  \n",
    "* Use divide and conquer to find the best values for hyperparameters. Start with something like this: [0, 0.25, 0.5, .75, 1]  \n",
    "* Assuming that the best value is 0.75, you need to narrow down your search to [0.5, 1] range like this: [0.5, .6, .7, .8, .9, 1]  \n",
    "* Assuming that the best value from previous step is 0.6, you should further narrow down the scope to [0.5, 0.7] range using the following: [.5, .54, .58, .62, .66, .7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for Regularization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark.ml.tuning as tune\n",
    "\n",
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols=[x for x in forest.columns if x != 'CoverType']\n",
    "    , outputCol='features')\n",
    "\n",
    "selector = feat.UnivariateFeatureSelector(\n",
    "    labelCol='CoverType'\n",
    "    , outputCol='selected'\n",
    "    , selectionMode = 'numTopFeatures'\n",
    "    ).setFeatureType(\"categorical\"\n",
    "    ).setLabelType(\"categorical\"\n",
    "    ).setSelectionThreshold(10) #select top 10 features\n",
    "\n",
    "logReg_obj = cl.LogisticRegression(\n",
    "    labelCol='CoverType'\n",
    "    , featuresCol=selector.getOutputCol()\n",
    "    , family='multinomial'\n",
    ")\n",
    "\n",
    "\n",
    "#use ParamGridBuilder to build a grid of parameters\n",
    "logReg_grid = (\n",
    "    tune.ParamGridBuilder()\n",
    "    #try 4 values for regParam\n",
    "    .addGrid(logReg_obj.regParam\n",
    "            , [0.0, .25, 0.5, .75, 1] \n",
    "#              [.1,.2, .3, .4] #if the best in the first round is .25 then drill down\n",
    "        )\n",
    "    #try 4 values for elasticNetParam\n",
    "    .addGrid(logReg_obj.elasticNetParam\n",
    "            , [0.0, .25, 0.5, .75, 1] \n",
    "#              [.6,.7,.8] #if the best in the first round is .75 then drill down\n",
    "        )\n",
    "    .build()\n",
    ")\n",
    "\n",
    "logReg_ev = ev.MulticlassClassificationEvaluator(\n",
    "    predictionCol='prediction'\n",
    "        , labelCol='CoverType')\n",
    "\n",
    "# use K-fold cross validation for grid search\n",
    "# CrossValidator binds all of these together\n",
    "# default value is k=3\n",
    "cross_v = tune.CrossValidator(\n",
    "    estimator=logReg_obj\n",
    "    , estimatorParamMaps=logReg_grid\n",
    "    , evaluator=logReg_ev\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[vectorAssembler, selector])\n",
    "data_trans = pipeline.fit(forest_train)\n",
    "\n",
    "logReg_modelTest = cross_v.fit(\n",
    "    data_trans.transform(forest_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best params - regParam: {}, elasticNetParam: {}'.format(\n",
    "    logReg_modelTest.bestModel._java_obj.getRegParam(),\n",
    "      logReg_modelTest.bestModel._java_obj.getElasticNetParam()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tuning_results = pd.DataFrame([list(x.values()) for x in logReg_grid], columns=['regParam', 'elasticNetParam'])\n",
    "tuning_results['F-1'] = logReg_modelTest.avgMetrics\n",
    "tuning_results.sort_values('F-1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure performance of best model\n",
    "data_trans_test = data_trans.transform(forest_test)\n",
    "results = logReg_modelTest.transform(data_trans_test)\n",
    "\n",
    "print(logReg_ev.evaluate(results, {logReg_ev.metricName: 'f1'}))\n",
    "print(logReg_ev.evaluate(results, {logReg_ev.metricName: 'weightedPrecision'}))\n",
    "print(logReg_ev.evaluate(results, {logReg_ev.metricName: 'weightedRecall'}))\n",
    "print(logReg_ev.evaluate(results, {logReg_ev.metricName: 'accuracy'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pretty\n",
    "selected_cols = ([x['name'] for x in data_trans_test.schema['selected'].metadata['ml_attr']['attrs']['numeric']])\n",
    "\n",
    "#print model coefficients\n",
    "#for a binomial model use coefficients instead of coefficientMatrix\n",
    "pd.set_option('display.max_columns', 10)\n",
    "ps.DataFrame(logReg_modelTest.bestModel.coefficientMatrix.toArray(), columns=selected_cols).to_spark().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = logReg_modelTest.bestModel.summary\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)\n",
    "\n",
    "# for multiclass, we can inspect metrics on a per-label basis\n",
    "print(\"False positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"True positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"Precision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "print(\"Recall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "print(\"F-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i, f))\n",
    "    \n",
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.tuning as tune\n",
    "\n",
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols=[x for x in forest.columns if x != 'CoverType']\n",
    "    , outputCol='features')\n",
    "\n",
    "selector = feat.UnivariateFeatureSelector(\n",
    "    labelCol='CoverType'\n",
    "    , outputCol='selected'\n",
    "    , selectionMode = 'numTopFeatures'\n",
    "    ).setFeatureType(\"categorical\"\n",
    "    ).setLabelType(\"categorical\"\n",
    "    ).setSelectionThreshold(10) #select top 10 features\n",
    "\n",
    "rf_obj = cl.RandomForestClassifier(\n",
    "    labelCol='CoverType'\n",
    "    , featuresCol=selector.getOutputCol()\n",
    "    , minInstancesPerNode=10\n",
    "    , numTrees=10\n",
    ")\n",
    "\n",
    "\n",
    "#use ParamGridBuilder to build a grid of parameters\n",
    "rf_grid = (\n",
    "    tune.ParamGridBuilder()\n",
    "    #try 4 values for minInstancesPerNode\n",
    "    .addGrid(rf_obj.minInstancesPerNode\n",
    "            , [5, 10, 20]\n",
    "        )\n",
    "    #try 4 values for maxDepth\n",
    "    .addGrid(rf_obj.maxDepth\n",
    "            , [10, 20]\n",
    "        )\n",
    "    .build()\n",
    ")\n",
    "\n",
    "rf_ev = ev.MulticlassClassificationEvaluator(\n",
    "    predictionCol='prediction'\n",
    "    , labelCol='CoverType')\n",
    "\n",
    "# use K-fold cross validation for grid search\n",
    "# CrossValidator binds all of these together\n",
    "# default value is k=3\n",
    "cross_v = tune.CrossValidator(\n",
    "    estimator=rf_obj\n",
    "    , estimatorParamMaps=rf_grid\n",
    "    , evaluator=rf_ev\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[vectorAssembler, selector])\n",
    "data_trans = pipeline.fit(forest_train)\n",
    "\n",
    "rf_modelTest = cross_v.fit(\n",
    "    data_trans.transform(forest_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best params - MaxDepth: {}, MinInstancesPerNode: {}'.format(\n",
    "    rf_modelTest.bestModel._java_obj.getMaxDepth(),\n",
    "      rf_modelTest.bestModel._java_obj.getMinInstancesPerNode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[list(x.values()) for x in rf_grid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tuning_results = pd.DataFrame([list(x.values()) for x in rf_grid], columns=['minInstancesPerNode', 'maxDepth'])\n",
    "tuning_results['F-1'] = rf_modelTest.avgMetrics\n",
    "tuning_results.sort_values('F-1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-validation splitting\n",
    "Instead of using k-fold cross validation, you can use a train-validation split approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_v = tune.TrainValidationSplit(\n",
    "    estimator=logReg_obj\n",
    "    , estimatorParamMaps=logReg_grid\n",
    "    , evaluator=logReg_ev\n",
    "    , trainRatio=0.75\n",
    "    #, parallelism=4\n",
    ")\n",
    "\n",
    "logReg_modelTrainV = (\n",
    "    train_v\n",
    "    .fit(data_trans.transform(forest_train)))\n",
    "\n",
    "results = logReg_modelTrainV.transform(data_trans_test)\n",
    "\n",
    "print(logReg_ev.evaluate(results, {logReg_ev.metricName: 'weightedPrecision'}))\n",
    "print(logReg_ev.evaluate(results, {logReg_ev.metricName: 'weightedRecall'}))\n",
    "print(logReg_ev.evaluate(results, {logReg_ev.metricName: 'accuracy'}))\n",
    "print('Best params, regParam: %s, elasticNetParam: %s' \n",
    "      %(logReg_modelTrainV.bestModel._java_obj.getRegParam(),\n",
    "      logReg_modelTrainV.bestModel._java_obj.getElasticNetParam()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print selected features from chisquare selector\n",
    "print(results.schema['selected'].metadata)\n",
    "\n",
    "#print model coefficients\n",
    "#for a binomial model use coefficients instead of coefficientMatrix\n",
    "logReg_modelTrainV.bestModel.coefficientMatrix.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
