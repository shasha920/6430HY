{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBA 6430 - Big Data Technology in Business\n",
    "# Dr. Mohammad Salehan\n",
    "# Feature Engineering with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T21:31:07.057495Z",
     "iopub.status.busy": "2022-06-28T21:31:07.057189Z"
    }
   },
   "source": [
    "As of Spark 2.0, the RDD-based APIs in the spark.mllib package have entered maintenance mode. The primary Machine Learning API for Spark is now the DataFrame-based API in the spark.ml package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get a cell_monitor error, you can ignore it. It is a Jypyter cell error and not a Spark error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will learn how to apply the following data preprocessing techniques using PySpark.\n",
    "* Dummies\n",
    "* Discretizing continuous variables\n",
    "* Standardization using z-score (i.e., normalization)<br/>\n",
    "You will also learn about Spark's vectorization pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates dummies for two categorical variables names `TYPE` and `CODE` using `ps.get_dummies()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "df = ps.DataFrame([\n",
    "    (1, \"A\", \"X1\"),\n",
    "    (2, \"B\", \"X2\"),\n",
    "    (3, \"B\", \"X3\"),\n",
    "    (1, \"B\", \"X3\"),\n",
    "    (2, \"C\", \"X2\"),\n",
    "    (3, \"C\", \"X2\"),\n",
    "    (1, \"C\", \"X1\"),\n",
    "    (1, \"B\", \"X1\"),\n",
    "], columns=[\"ID\", \"TYPE\", \"CODE\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = ps.get_dummies(df, columns=[\"TYPE\", \"CODE\"], drop_first=False)\n",
    "dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = ps.get_dummies(df, columns=[\"TYPE\", \"CODE\"], drop_first=True)\n",
    "dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretizing continuous variables using quantiles\n",
    "If you suspect that some features have a nonlinear relationship with your outcome variable, you can consider discritizing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ps.set_option('plotting.backend', 'matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "signal_df = ps.read_csv('s3://cis4567-salehan/Spark/Data/fourier_signal.csv')\n",
    "signal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "signal_df.describe()\n",
    "#the mean is almost zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_df.plot.line(y='signal')\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark.ml.feature as feat\n",
    "steps = feat.QuantileDiscretizer(\n",
    "       numBuckets=10,\n",
    "       inputCol='signal',\n",
    "       outputCol='discretized')\n",
    "\n",
    "#.pandas_api() is the same as .to_pandas_on_spark() which has been deprecated\n",
    "transformed = (\n",
    "    steps\n",
    "    .fit(signal_df.to_spark())\n",
    "    .transform(signal_df.to_spark())\n",
    ").pandas_api()\n",
    "transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "transformed.plot.line(y='discretized')\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "* Almost exclusively, every estimator (or, in other words, an ML model) found in the MLlib module expects to see a single column as an input; the column should contain all the features a data scientist wants such a model to use. \n",
    "* The `.VectorAssembler(...)` method, as the name suggests, collates multiple features into a single column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorAssembler = (\n",
    "    feat.VectorAssembler(\n",
    "        inputCols=['signal', 'discretized'], \n",
    "        outputCol='feat'\n",
    "    )\n",
    ")\n",
    "\n",
    "#the 5 feature output of the record from PCA\n",
    "signal_vectorized  = vectorAssembler.transform(transformed.to_spark()).pandas_api()\n",
    "signal_vectorized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vec = feat.VectorAssembler(\n",
    "    inputCols=['signal']\n",
    "    , outputCol='signal_vec'\n",
    ")\n",
    "\n",
    "signal_vectorized  = vec.transform(signal_df.to_spark())\n",
    "\n",
    "norm = feat.StandardScaler(\n",
    "    inputCol=vec.getOutputCol()\n",
    "    , outputCol='signal_norm'\n",
    "    , withMean=True\n",
    "    , withStd=True\n",
    ")\n",
    "\n",
    "signal_norm = (\n",
    "    norm\n",
    "    .fit(signal_vectorized)\n",
    "    .transform(signal_vectorized)\n",
    ").pandas_api()\n",
    "\n",
    "signal_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "signal_norm = signal_norm.to_spark().select('signal', \n",
    "                                 vector_to_array('signal_norm')[0].alias('signal_norm')\n",
    "                             ).pandas_api()\n",
    "signal_norm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "fig.tight_layout(pad=1.5)\n",
    "for i, col in enumerate(['signal', 'signal_norm']):\n",
    "    signal_norm.plot.line(ax=ax[i], y=col, title=col)\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "* The Pipeline class helps to sequence, or streamline, the execution of separate blocks that\n",
    "lead to an estimated model; it chains multiple Transformers and Estimators to form a\n",
    "sequential execution workflow.\n",
    "* Pipelines are useful as they avoid explicitly creating multiple transformed datasets as the\n",
    "data gets pushed through different parts of the overall data transformation and model\n",
    "estimation process. \n",
    "* Instead, Pipelines abstract distinct intermediate stages by automating\n",
    "the data flow through the workflow. \n",
    "* This makes the code more readable and maintainable\n",
    "as it creates a higher abstraction of the system, and it helps with code debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "vec = feat.VectorAssembler(\n",
    "    inputCols=['signal']\n",
    "    , outputCol='signal_vec'\n",
    ")\n",
    "\n",
    "norm = feat.StandardScaler(\n",
    "    inputCol=vec.getOutputCol()\n",
    "    , outputCol='signal_norm'\n",
    "    , withMean=True\n",
    "    , withStd=True\n",
    ")\n",
    "\n",
    "norm_pipeline = Pipeline(stages=[vec, norm])\n",
    "signal_norm = (\n",
    "    norm_pipeline\n",
    "    .fit(signal_df.to_spark())\n",
    "    .transform(signal_df.to_spark())\n",
    ").pandas_api()\n",
    "\n",
    "signal_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
